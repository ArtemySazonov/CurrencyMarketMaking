\section{Online machine learning and reinforcement learning}\label{section:OMLandRL}
    In recent years, online machine learning and reinforcement learning have gained significant attention in the field of optimal trade execution. These approaches leverage the power of algorithms and data to continuously learn and adapt execution strategies in real-time, with the goal of achieving optimal trade outcomes.

    Online machine learning, also known as incremental or streaming machine learning, allows for the learning process to occur as new data becomes available. Unlike batch learning, where the algorithm is trained on a fixed dataset, online learning algorithms update their models iteratively, dynamically incorporating new observations into the learning process. This flexibility makes online machine learning well-suited for trade execution, where market conditions change rapidly and the ability to adapt quickly is crucial.

    Reinforcement learning, a subset of online machine learning, is particularly relevant in the context of optimal trade execution. Reinforcement learning algorithms learn through trial and error interactions with an environment, aiming to maximize a reward signal. In the case of trade execution, the algorithm interacts with the market and aims to optimize execution performance by maximizing rewards such as profitability, minimizing transaction costs, or reducing market impact.

    One key advantage of reinforcement learning is its ability to handle complex, dynamic environments without relying on pre-defined rules or models. Reinforcement learning agents learn from experience, exploring different execution strategies and adjusting their actions based on the feedback received. This adaptability allows them to capture non-linear relationships and exploit patterns in the market, potentially leading to improved execution performance.

    However, applying online machine learning and reinforcement learning to trade execution also poses challenges. One of the primary challenges is data quality and availability. To train and fine-tune algorithms effectively, high-quality data that accurately represents market conditions is essential. Additionally, the presence of noisy or incomplete data can impact the learning process and the reliability of the execution strategies.

    Furthermore, the selection of appropriate features and state representation is critical in capturing the relevant information for trade execution. Different market factors, such as liquidity, volatility, and order book dynamics, should be carefully considered to construct informative features that drive the learning process.

    Another challenge is the delicate balance between exploration (trying out new actions to gather data and learn) and exploitation (leveraging learned knowledge for optimal decision-making). In trade execution, this balance becomes crucial, as excessive exploration may result in increased risk or missed trading opportunities, while excessive exploitation may hinder the agent from adapting to changing market conditions.

    Moreover, the deployment of online machine learning or reinforcement learning algorithms in real-time trading environments introduces additional concerns. The algorithms must be able to handle high-frequency data, process information quickly, and make decisions within tight time constraints. Furthermore, risk management and constraints, such as obtaining desired trade volumes within specified time horizons or complying with regulatory requirements, need to be incorporated into the algorithm design.

    Despite these challenges, the application of online machine learning and reinforcement learning to optimal trade execution holds promise. The ability to learn and adapt in real-time, exploit market patterns, and optimize trade outcomes makes these approaches appealing for market participants aiming to enhance their execution strategies. As research and technology continue to advance, the integration of online machine learning and reinforcement learning techniques into trade execution algorithms is expected to be an active area of exploration and development.

    In the realm of online machine learning and reinforcement learning, another powerful tool used for optimizing trade execution is Markov Decision Processes (MDPs). MDPs provide a mathematical framework for modeling decision-making processes in stochastic environments, where the outcomes of actions are uncertain. 
    Formally, an MDP is defined by a tuple $(\mathcal S, \mathcal A, P, R)$, where:
    \begin{itemize}
        \item $\mathcal S$ represents the set of states in the environment;
        \item $\mathcal A$ represents the set of actions that can be taken;
        \item $P$ is the transition function, which defines the probability distribution of transitioning from one state to another after taking a specific action;
        \item $R$ is the reward function, which quantifies the desirability or utility of being in a particular state or taking a specific action.
    \end{itemize}

    The goal in an MDP is to find a policy that maximizes the expected cumulative reward over time. A policy, denoted by $\pi$ or $\nu$, is a (possibly, stochastic) mapping from states to actions, indicating the action to be taken in each state. 

    In the context of optimal trade execution, Markov Decision Processes provide a framework for modeling the dynamic nature of financial markets, considering the uncertainties involved in price fluctuations and trade executions. By formulating the trade execution problem as an MDP, reinforcement learning algorithms can be applied to learn optimal trading strategies that maximize expected profits while minimizing transaction costs.


\section{Electronic market making}\label{section:ElectronicMM}
    We need to understand the core of the optimal execution problem in the market. 
    As it was stated in \cite{Cartea2015} and \cite{Bouchaud2018},
    the objective of the electronic market is to match the buy and sell orders 
    of the market participants. In practice we can divide all orders to 
    the two groups: \emph{market orders} (MO) and \emph{limit orders} (LO). 
    \begin{definition}
        A \emph{limit order} is an order to buy or sell a security at a specific price or better. This type of order guarantees the execution price, but does not guarantee the execution itself.
    \end{definition}
    \begin{definition}
        A \emph{market order} is an order to buy or sell a security immediately. This type of order guarantees that the order will be executed, but does not guarantee the execution price.
    \end{definition}
    
    The limit orders are \emph{passive}, i.e. they are not executed immediately, since 
    we need to find a counterparty for them\footnote{The limit order can be executed immediately if it's limit price is higher than the best counterorder}. The market orders are \emph{aggressive},
    i.e. they are executed immediately, since they are the ones being dynamically matched with the limit orders. 
    The market orders for $N$ shares collects the best $N$ shares limit orders from the order book.
    The order book stores only the information about the limit orders, so we can call it the limit order book (LOB).
    If the market order is larger than the best priced limit order, then the remaining part of the 
    market order is executed via the second best limit order from the LOB. Because of that, the current 
    market price of the asset moves in the direction of the market order.

    This effect is called a \emph{price impact} and it is the main reason why we should need to 
    optimize the trade execution. The price impact could be represented as a function of the order size and the time.
    If we execute the trade quickly, then we will pay a lot of money for the price impact. We should 
    note that the price impact has the inverse relation with the liquidity of the asset. The more liquid the asset is, the less price impact is.
    If we execute only the best priced limit orders in order to avoid the price impact of the certain assets, then we are 
    doomed to be executing the MO for quite a long time, what significantly increases the \emph{market risk}. The market 
    risk is the risk of the drastic price change during the execution horizon. Therefore, we found a trade-off between the 
    price impact and the market risk.
    
    Now we are ready to formulate the \emph{optimal execution problem}: liquidate the position by minimizing the functional of the price impact and the market risk. 
    Let us write this problem in terms of stochastic optimal control.

    Let us have $W$ lots of an asset in the long position that we need to sell during time $T$ (measured in ticks) from now.
    There are $L$ liquidation intervals and we define the \emph{liquidation times} $t_k = k\tau$, $\tau = T/L$.
    \begin{definition}\label{definition:tradingtrajectory}
        The \emph{trading trajectory} is a process $(w_k)_{k = 0, \dots, L}$, where $w_k$ is a number of lots we 
        still posess at time $t_k$. Alternatively, we can define the \emph{trade list} $n_k = \Delta w_k$, $k = 1, \dots, L$ as a 
        number of lots we sell at time $t_k$.
        The \emph{trading strategy} is a rule for determining $n_k$ given the information avaliable at time $t_k$. Mathematically speaking,
        \begin{equation*}
            \hat n_k = \mathbb{E}^\nu\left[n_k\vert \mathcal{F}_{t_{k-1}}\right],
        \end{equation*} 
        where $\nu$ is the probability measure which corresponds to the trading strategy.
    \end{definition}
    We can divide the strategies to \emph{static} (deterministic, all the parameters are known upon the start of the execution) and \emph{dynamic} (stochastic).
    Static strategies do not require any learning, but for the dynamic strategies it is often useful to use online machine learning (RL in particular).
    As it was stated in \cite{Cartea2015}, we can differentiate all dynamic traders (same as their correspoding strategies) into the three main classes:
    \begin{enumerate}
        \item \emph{Fundamental traders} (also \emph{noise \emph{or} liquidity traders}): those traders, who exploit some general exogenous economic factors. There is a subtle difference between all three: \begin{itemize}
            \item Fundamental traders usually have some kind of medium or long term strategy,
            \item Noise traders are those who trade orthogonally to the market events, i.e. their actions weakly depend on others,
            \item Liquidity traders are those, who are forced (by their strategy) to exploit the market orthogonally to major events;
        \end{itemize}
        \item \emph{Informed traders}: those traders, who profit from some insider knowledge, a.k.a. information, which is not reflected in the prices;
        \item \emph{Market makers}: professional traders, owners of the market power;
        \item Sometimes, the \emph{arbitrageurs} are differentiated as a fourth group. However, they might be considered to be a subclass of the informed traders.
    \end{enumerate}

\section{Optimal Execution Algorithms}\label{section:OptExecAlgos}
    \subsection{Preliminary Information}
        First of all, we can neglect the time value of money since we are working in the high-frequency
        trading world, i.e. the money do not lose any value in our scale. Therefore, $\gamma \equiv 1$.

        Our trading bot has the following set of actions:
        \begin{itemize}\label{page:actions}
            \item Hold;
            \item Sell 1 lot of the asset;
            \item Sell 2 lots of the asset;
            \item Sell 3 lots of the asset;
            \item Sell 4 lots of the asset;
            \item etc.
        \end{itemize}
        The number of possible actions depends on the leftover size of the position to be liquidated.
        The algorithm stops when there is no other action but to hold the position with 0 assets. 
        The cumulative regret function is the accumulated price impact costs with the correction for the excessive risk.
        It is obvious that the target functional without the risk correction has its $\epsilon$-optimal 
        solution with the uniform distribution of the trades over the given liquidation horizon.

    \subsection{Time-Weighted Average Price Algorithm}
        We shall use the \emph{Time-Weighted Average Price algorithm} (TWAP) execution algorithm as a baseline model.
        According to \cite{TWAP}, 
        \begin{quote}
            TWAP trading algorithms seek to optimize a trade's average price while executing over a specified time period. This is 
            generally used to execute large orders that are expected to have significant market impact.
        \end{quote}
        We use a modification of the classical TWAP algorithm (see Algorithm \ref{algorithm:stwap})
        \begin{algorithm}
            \caption{M-TWAP Algorithm}
            \begin{algorithmic}
                \State Initialize the trading list \mintinline{python}|n = [1, ..., 1] * int(W/L)| of length \mintinline{python}|L|
                \State Add \mintinline{python}|W - int(W/L)*L| lots to the last element of the trading list.
            \end{algorithmic}
            \label{algorithm:stwap}
        \end{algorithm}

    \subsection{Almgren-Chriss Algorithm}
        The \emph{Almgren-Chriss algorithm} (AC) was introduced in \cite{Almgren2000}.

        \begin{definition}
            The \emph{capture} of the trajectory is the total nominal trading revenue upon completion of the execution:
            \begin{equation*}
                CP(n, S) = \sum_{k=1}^{L} n_kS_k.
            \end{equation*}
            The \emph{total cost} of the trading trajectory is the difference between the capture and the initial book value:
            \begin{equation*}
                TC = XS_0 - CP(n, S).
            \end{equation*}
        \end{definition}
        \noindent$\lambda$ is the relative risk propensity:
        \begin{equation*}
            \lambda(w) = -\frac{u''(w)}{u'(w)},
        \end{equation*}
        where $u$ is the utility function of the trader. If our initial portfolio is fully owned, then as we transfer our assets from the 
        risky stock into the alternative investment, total wealth remains roughly constant, and we may take $\lambda$ to be constant throughout our trading period.
        
        Let $w_l$ and $A_l$ be the number of units planned to be held and sold at the time slot $l\in\mathcal L$.
        We denote $g(A_l)$ and $h(A_l)$ the permanent and temporary price impact functions.
        The AC model assumes that the stock price follows an arithmetic random walk with independent increments.
        The cost of trading is called an \emph{implementation shortfall}:
        \begin{equation*}
            \operatorname{IS} = Wp_b(1) - \sum_{l=1}^L p_b(l) = \sum_{l=1}^L \left(g(A_l)w_l + h(A_l)A_l\right) - \sum_{l=1}^L \sigma \zeta_lw_l.
        \end{equation*}
        $\operatorname{IS}$ is normally distributed if $\zeta_.$ are normally distributed. The objective of the AC algorithm is to solve the following optimizational problem:
        \begin{equation}\label{equation:ACtarget}
            \mathbb{E}\left[\operatorname{IS}\right] + \lambda \var \operatorname{IS} \to \min.
        \end{equation}
        If $\lambda > 0$, then the strategy becomes risk-averse: we try to select actions which do not dramatically change the variance.
        The optimal policy for the case of the linear impact functions: $g(A_l) = \gamma A_l$ and $h(A_l) = \eta A_l$:
        \begin{equation*}
            A_l^* = \frac{2\sinh \frac{\kappa}{2}}{\sinh \kappa L}\cosh\left(\kappa (L-l+0.5)\right) W,
        \end{equation*}
        where
        \begin{equation*}
            \kappa = \cosh^{-1}\left(0.5\left(\frac{\lambda\sigma}{\eta - 0.5\gamma}\right)^2 + 1\right).
        \end{equation*}

    \subsection{Greedy exploitation in Limit Order Book Execution Algorithm}
        The \emph{Greedy exploitation in Limit Order Book Execution algorithm} (GLOBE) was introduced in \cite{Akbarzadeh2018}.
        It is based on the a model-based reinforcement learning approach using the Markov decision processes (MDP) framework with bounded regret. We found some inaccuracies in the original 
        article, so we modified the algorithm in order to make it suitable for using it in the real market.

        \subsubsection{Original algorithm}
            For now, let us describe the setup from the original article:
            \begin{enumerate}
                \item The system consists of a finite state of states $\mathcal S = \mathcal I \times \mathcal M$, where $\mathcal I$ and $\mathcal M$ are the \emph{private states} and \emph{market states} correspodingly:
                \begin{itemize}
                    \item $\mathcal I = \{0,\dots, W_{\text{max}}\}$ is a set of the inventory levels. The inventory level of shares at the beginning of each round
                        is between $W_{\text{min}}$ and $W_{\text{max}}$, $0 < W_{\text{min}} \leq W_{\text{max}}$. The private state at time slot $l$ of round $\rho$ 
                        is denoted by $I_l^\rho$. We also assume that $I_1^\rho = W_\rho \in \mathcal I$ is the initial inventory.
                    \item The market state $\mathcal M$ is a set of integers that are used to define the dynamics of the bid prices. Let $M_l^\rho\in \mathcal M$ be 
                        the market state and $p_b(\rho, l)$ is the bid price in the time slot $l$ at round $\rho$. They have the following relation:
                        \begin{equation}
                            M_l^\rho = \frac{p_b(\rho, l) - p_b(\rho, 1)}{\sigma_\rho},
                        \end{equation}
                        where $\sigma_\rho$ is the bid price volatility. 
                        The reasoning behind this formula is that we can use the knowledge from the past to predict the bid price movement even when the LH interval is different during the current round.
                        Similarly, we introduce the $p_a(\rho, l)$ (ask price) and $p_m(\rho, l)$ (mid price).
                        The returns of the asset are defined as follows:
                        \begin{equation*}
                            \operatorname*{Ret} (\rho) = \log \frac{p_m(\rho, l)}{p_m(\rho, 1)} .
                        \end{equation*}
                        We define the volatility of the returns as
                        \begin{equation*}
                            \sigma_\rho = \sqrt{\frac{\sum_{j=1}^{\rho-1} \left(\operatorname*{Ret}(j) - \mu_\rho\right)^2}{\rho-1}},
                        \end{equation*}
                        where 
                        \begin{equation*}
                            \mu_\rho = \frac{1}{\rho-1}\sum_{j=1}^{\rho-1}\operatorname*{Ret}(j).
                        \end{equation*}
                    \item We define the joint state in time slot $l$ of round $\rho$ as $S_l^\rho = (I_l^\rho, M_l^\rho)$.
                    \item The action set is defined as $\mathcal A = \{0, \dots, W_{\text{max}}\}$, where $a_l^\rho \in \mathcal A$ is the action taken in time slot $l$ of round $\rho$. For more details, check page \pageref{page:actions}. In each round, a sequence of actions is selected with the aim
                    of maximizing the revenue.
                \end{itemize}
            \end{enumerate}

            \begin{algorithm}
                \caption[GLOBE Algorithm]{Greedy exploitation in Limit Order Book Execution (GLOBE)}
                \begin{algorithmic}
                    \State Input: $L, \mathcal{M}, \mathcal{M}^r$
                    \State Initialize: $\rho = 1, N(M)=0, N(M,M)=0, \forall M \in \mathcal{M}^r , \forall M' \in \mathcal{M}$
                    \While {$\rho > 1$}
                        \State $\hat P_\rho (M, M') := \frac{N(M, M') + \1(N(M) = 0)}{N(M) + |\mathcal M| \1 (N(M) = 0)}$
                        \State Update $\sigma_\rho$ in the AC model based on the past observations
                        \State Observe $X_\rho = (W_\rho, p_r(\rho), \sigma_\rho, B_1^\rho)$
                        \State Compute $A_l$ based on the AC model $\forall l \in \mathcal{L}$
                        \State Compute the estimated optimal policy by dynamic programming using the action set $\mathcal A_l^*$, $\forall l \in \mathcal{L} - \{L\}$ and $\hat P_\rho (M, M')\ \forall M \in \mathcal{M}^r, \forall M' \in \mathcal M$ according to \eqref{globe:rule3}
                        \State $I_1^\rho = W_\rho$, $l=1$
                        \For {$l = 1$; $l < L$; $l\ +\!\!=1$}
                            \State Observe $M_l^\rho$, sell $a_l^\rho\in \mathcal{A}^*_l$ using the estimated policy
                            \State Calculate $C_{X_\rho}(M_l^\rho, a_l^\rho)$
                            \State $I_{l+1}^\rho = I_l^\rho - a_l^\rho$
                        \EndFor
                        \State $a_{L}^\rho := I_L^\rho$
                        \State $\rho\ +\!\!= 1$
                        \State Update $N(M, M')$ and $N(M)$ $\forall M \in \mathcal{M}^r$ according to \eqref{globe:rule1} and \eqref{globe:rule2}
                    \EndWhile
                \end{algorithmic}
                \label{algorithm:GLOBE}
            \end{algorithm}
            
            \begin{align}
                & N_\rho(M, M') = \sum_{\rho' = 1}^{\rho-1} \sum_{l = 1}^{L-1} \1\left(M_{\rho'}^l = M\right) \1\left(M_{\rho'}^{l+1} = M'\right); \label{globe:rule1}\\
                & N_\rho(M) = \sum_{M'\in\mathcal{M}}N_\rho(M, M'); \label{globe:rule2}\\
                & \hat P_\rho(M, M') = \frac{N_\rho(M, M') + \1\left(N_\rho(M) = 0\right)}{N_\rho(M) + |\mathcal M|\1\left(N_\rho(M) = 0\right)}. \label{globe:rule3}
            \end{align}

        \subsubsection{Our algorithm}
            The changes we proposed:
            \begin{enumerate}
                \item The authors of the article did not provide a clear way to estimate the set of reachable states $\mathcal{M}^r$. Therefore, we changed it to $\mathcal{M}$. The complexity of the algorithm slightly increased.
                \item The \texttt{init\_sigma} is not correctly defined: division by zero. We estimate the initial volatility by the historical data. Nothing changed since the second round will overwrite the initial guess.
                \item 
            \end{enumerate}
            \begin{algorithm}
                \caption[Modified GLOBE Algorithm]{Modified Greedy exploitation in Limit Order Book Execution (M-GLOBE)}
                \begin{algorithmic}
                    \State Input: $L, \mathcal{M}$
                    \State Initialize: $\rho = 1, N(M)=0, N(M,M)=0, \forall M, M' \in \mathcal{M}$
                    \While {$\rho > 1$}
                        \State $\hat P_\rho (M, M') := \frac{N(M, M') + \1(N(M) = 0)}{N(M) + |\mathcal M| \1 (N(M) = 0)}$
                        \State Update $\sigma_\rho$ in the AC model based on the past observations
                        \State Observe $X_\rho = (W_\rho, p_r(\rho), \sigma_\rho, B_1^\rho)$
                        \State Compute $A_l$ based on the AC model $\forall l \in \mathcal{L}$
                        \State Compute the estimated optimal policy by dynamic programming using the action set $\mathcal A_l^*$, $\forall l \in \mathcal{L} - \{L\}$ and $\hat P_\rho (M, M')\ \forall M, M' \in \mathcal M$ according to \eqref{globe:rule3}
                        \State $I_1^\rho = W_\rho$, $l=1$
                        \For {$l = 1$; $l < L$; $l\ +\!\!=1$}
                            \State Observe $M_l^\rho$, sell $a_l^\rho\in \mathcal{A}^*_l$ using the estimated policy
                            \State Calculate $C_{X_\rho}(M_l^\rho, a_l^\rho)$
                            \State $I_{l+1}^\rho = I_l^\rho - a_l^\rho$
                        \EndFor
                        \State $a_{L}^\rho := I_L^\rho$
                        \State $\rho\ +\!\!= 1$
                        \State Update $N(M, M')$ and $N(M)$ $\forall M \in \mathcal{M}$ according to \eqref{globe:rule1} and \eqref{globe:rule2}
                    \EndWhile
                \end{algorithmic}
                \label{algorithm:MGLOBE}
            \end{algorithm}

    \subsection{Metrics for the backtest}
        \begin{definition}[Average Cost Per Round]
            \begin{align}
                & \operatorname*{ACPR} = \frac{1}{\operatorname{num\_of\_rounds}} \sum_\rho IS_\rho,\\
                & IS_\rho = \left( W  p_m(\rho, 1) - \sum_l \frac{A^\rho_l p_b(\rho, l)}{W p_m(\rho, 1)}\right).\nonumber
            \end{align}
        \end{definition}
        \begin{definition}[Relative Improvement per round]
            $\operatorname*{RI}$ shows the relative decrease in the trading costs compared to the baseline model (TWAP, in our case):
            \begin{equation}
                \operatorname*{RI} = \frac{\operatorname*{ACPR}_{\text{TWAP}} - \operatorname*{ACPR}_{\text{Algorithm}}}{\operatorname*{ACPR}_{\text{TWAP}}}.
            \end{equation}
        \end{definition}

\section{Practical Problems and Backtesting}\label{section:Backtest}
    \subsection{Data collection and features calculation}
        We obtained the L3 market data for the four currency pairs (*\_TOM\footnote{TOD is a transaction in which currency conversion is performed on the day of the transaction.
        TOM is a transaction in which the conversion is made the day after the conclusion of the transaction, while the conversion rate is fixed on the day of the transaction.
        SPOT is similar to the TOM transaction, but the conversion is made on the third day after the transaction is concluded.} instrument) from the Moscow Exchange:
        \begin{enumerate}
            \item USD/RUB; 
            \item EUR/RUB; 
            \item USD/CNH;
            \item EUR/USD.
        \end{enumerate} 
        Technical data description and order book implementation details could be found in the Appendix \ref{appendix:data}.
        To calibrate the trading algorithms, we calculated the following parameters from the LOB:
        \begin{itemize}
            \item price-by-volume $P(Q)$;
            \item volume-by-price-threshold $Q(dP)$, $dP$ is the maximal possible price impact;
            \item Imbalance of the previous parameters\begin{itemize}
                \item $I_1(Q) = (\texttt{Ask}(Q) - \texttt{Ask}(0)) - (\texttt{Bid}(0) - \texttt{Bid}(Q))$,
                \item $I_2(dP) = Q(\texttt{Ask} - \texttt{BestAsk}) - Q(\texttt{Bid} - \texttt{BestBid})$.
            \end{itemize}
        \end{itemize}

    \subsection{Backtesting the algorithms}\label{section:backtest}
        Backtesting steps:
        \begin{enumerate}
            \item We chose some intraday data for all four instruments.
            \item We chose every 100th tick of the book for the L3 intraday data (we did not assume the overnights or execution longer than the rest of the trading day) assuming that the liquidity comes back to its' historical level after 100 ticks.
            \item We calculated the ACPR series in consecutive rounds for the obtained time series of functions.
            \item We calculated the RI metrics and plotted the results.
        \end{enumerate}
        Note that the backtesting uses the morning data only due to the tendency to higher liquidity at this time of day. 
        This is vital since the authors of the GLOBE algorithm emphasize that the size of the position to be liquidated should be quite small compared to the depth of the limit order book.

        For now, we shall deeply analyze the USD/CNH\_TOM only (Figures \ref{fig:backtest1} -- \ref{fig:backtest4}). The complete backtest for the USD/RUB\_TOM could be found in Appendix \ref{appendix:backtest}.
        We used the following parameters of the models:
            \begin{minted}[gobble=12]{python3}
                T = 50
                W = np.ones(shape=10, dtype=int) * 500
    
                W_max = 501
    
                lamb = 2 * (0.01)
                eta = 2 * (0.001)
                init_sigma = 0.021713089727230926
    
                start = 2
                stop = T
                num = 49
    
                skip = 0
                rounds_for_est = 15
    
                K = 100000.0
            \end{minted}

        We can clearly see that due to the incorrectly calibrated parameters the GLOBE family of algorithms performs very poorly. 
        It is observable that for all chosen trading days GLOBE+ tends to outperform the standard GLOBE algorithm, but they both perform poorly compared to TWAP and AC algorithms.
        \begin{figure}[htbp]
            \includegraphics[width=\textwidth]{USD_CNH_T+1 2022-10-04 T = 50 W = 500.pdf}
            \caption{ACPR and RI metrics for USD\_CNH\_T+1 2022-10-04 at 50 with $W = 500$}\label{fig:backtest1}
        \end{figure}
        
        \begin{figure}[htbp]
            \includegraphics[width=\textwidth]{USD_CNH_T+1 2022-10-12 T = 50 W = 500.pdf}
            \caption{ACPR and RI metrics for USD\_CNH\_T+1 2022-10-12 at 50 with $W = 500$}
        \end{figure}
        
        \begin{figure}[htbp]
            \includegraphics[width=\textwidth]{USD_CNH_T+1 2022-10-21 T = 50 W = 500.pdf}
            \caption{ACPR and RI metrics for USD\_CNH\_T+1 2022-10-21 at 50 with $W = 500$}
        \end{figure}
        
        \begin{figure}[htbp]
            \includegraphics[width=\textwidth]{USD_CNH_T+1 2022-10-31 T = 50 W = 500.pdf}
            \caption{ACPR and RI metrics for USD\_CNH\_T+1 2022-10-31 at 50 with $W = 500$}\label{fig:backtest4}
        \end{figure}